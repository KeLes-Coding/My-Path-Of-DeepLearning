{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.autograd的简要介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.autograd是 PyTorch 的自动差分引擎，可为神经网络训练提供支持。 在本节中，您将获得有关 Autograd 如何帮助神经网络训练的概念性理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 背景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络（NN）是在某些输入数据上执行的嵌套函数的集合。 这些函数由参数（由权重和偏差组成）定义，这些参数在 PyTorch 中存储在张量中。\n",
    "\n",
    "训练 NN 分为两个步骤：\n",
    "\n",
    "**正向传播**：在正向传播中，NN 对正确的输出进行最佳猜测。 它通过其每个函数运行输入数据以进行猜测。\n",
    "\n",
    "**反向传播**：在反向传播中，NN 根据其猜测中的误差调整其参数。 它通过从输出向后遍历，收集有关函数参数（梯度）的误差导数并使用梯度下降来优化参数来实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 在 PyTorch 中的用法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们来看一个训练步骤。 对于此示例，我们从torchvision加载了经过预训练的 resnet18 模型。 我们创建一个随机数据张量来表示具有 3 个通道的单个图像，高度&宽度为 64，其对应的label初始化为一些随机值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "data = torch.rand(1, 3, 64, 64) # 一个随机数据张量来表示具有 3 个通道的单个图像，高度&宽度为 64\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们通过模型的每一层运行输入数据以进行预测。 这是**正向传播**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.5327e-01, -3.6561e-01, -4.8504e-01, -1.5568e+00, -8.2300e-01,\n",
      "          1.2818e-01, -3.3972e-01,  7.4164e-01,  5.8094e-01, -1.1761e+00,\n",
      "         -1.0778e+00, -6.9845e-01, -8.5295e-03, -1.0883e+00, -1.1317e+00,\n",
      "         -6.3589e-01, -5.5289e-01, -4.6761e-01, -7.9638e-01, -4.2396e-01,\n",
      "         -1.5664e+00, -6.5198e-01, -1.5454e+00,  3.7898e-02, -9.4030e-01,\n",
      "         -1.0903e+00, -5.8979e-01, -1.0924e+00, -7.3947e-01, -2.6233e-01,\n",
      "         -7.0595e-01, -9.0380e-01, -4.2306e-01, -3.6932e-01, -1.1448e-01,\n",
      "         -5.1988e-01,  7.1412e-01, -8.1132e-01, -3.9527e-01,  8.9076e-03,\n",
      "         -7.0364e-01, -9.5917e-01, -1.2636e+00, -2.8514e-01, -6.6501e-01,\n",
      "         -4.3397e-01, -9.0125e-01, -5.1765e-01, -1.1198e+00, -1.1883e+00,\n",
      "         -4.7037e-01,  3.3842e-01, -4.7288e-01, -5.7300e-01, -1.8765e-01,\n",
      "         -1.3400e+00, -3.0780e-01, -1.5653e+00, -6.5409e-01, -3.5774e-01,\n",
      "          9.5960e-01,  8.3605e-02, -1.6788e-01, -4.5031e-02, -9.0750e-01,\n",
      "         -1.8984e-01, -3.5132e-01, -3.0175e-01, -9.0267e-01, -1.1354e+00,\n",
      "         -1.4325e+00,  3.0192e-01, -1.4619e+00, -2.0592e-01, -1.0655e+00,\n",
      "         -9.9808e-01,  2.8613e-01, -5.1195e-01,  5.7530e-01,  3.6337e-02,\n",
      "         -5.0205e-01, -1.4375e+00,  2.4086e-01, -3.8256e-01, -6.0668e-01,\n",
      "          1.7839e-01,  3.8809e-01,  4.8788e-01, -5.7432e-02, -5.9473e-01,\n",
      "         -9.2075e-01, -1.2443e+00, -1.8482e+00, -4.0965e-01,  2.6074e-03,\n",
      "         -2.4457e+00, -7.0305e-01, -4.5696e-01, -1.4183e+00, -2.5055e-01,\n",
      "         -1.0034e+00, -9.2490e-01, -8.4505e-01,  3.3159e-02, -9.2991e-02,\n",
      "         -7.2708e-01, -4.6133e-01, -1.3246e+00, -9.4169e-01, -1.3573e+00,\n",
      "         -8.8737e-01, -5.7058e-01,  1.1395e+00,  4.3569e-01,  5.6080e-01,\n",
      "         -1.1055e+00, -7.7210e-01, -4.2957e-01,  7.6868e-01, -3.1396e-01,\n",
      "         -7.6659e-01,  1.6924e-01,  3.4151e-01,  2.8277e-01,  9.8899e-01,\n",
      "         -5.2183e-02,  3.9910e-01, -1.6826e+00, -1.4988e+00, -1.1344e+00,\n",
      "         -1.7135e+00, -1.6352e+00, -1.1920e+00, -1.3217e+00, -5.4774e-01,\n",
      "         -1.3400e+00, -8.2730e-01, -1.2021e+00, -1.4071e+00, -1.6886e+00,\n",
      "         -1.4321e+00, -1.7736e+00, -2.0470e+00, -1.7216e+00, -8.9592e-01,\n",
      "         -2.4742e-01, -1.1133e+00, -1.8528e+00, -1.0799e+00, -1.2754e+00,\n",
      "          4.9983e-01,  1.5676e+00, -8.9800e-01, -5.8727e-01,  6.2832e-02,\n",
      "          1.6071e-01, -2.1671e-01, -8.2170e-02,  9.0989e-02,  2.4591e-01,\n",
      "          2.7819e-01,  6.0128e-01,  6.4363e-02,  6.5385e-01,  2.7512e-01,\n",
      "         -3.2716e-01, -3.3865e-01, -4.6424e-01,  5.6603e-01, -6.7928e-01,\n",
      "         -2.1372e-02,  7.6542e-01,  3.2389e-01,  1.4555e-01,  3.4943e-02,\n",
      "         -7.3130e-01, -1.4738e-01, -8.9482e-02,  6.7025e-01,  6.6056e-01,\n",
      "          6.3904e-01, -3.3042e-01,  7.9714e-01,  2.1639e-02,  6.7408e-01,\n",
      "          7.0390e-01,  7.8546e-01,  1.4240e-01, -4.9364e-03,  5.2686e-01,\n",
      "         -4.7818e-01,  2.9301e-01,  4.6797e-01,  4.4100e-01, -6.3018e-01,\n",
      "          7.3599e-01,  4.9950e-02,  1.7220e-01,  2.4879e-01,  7.0978e-01,\n",
      "          9.9165e-02,  5.8325e-02,  3.7635e-01,  6.3189e-01,  2.1507e-01,\n",
      "          2.1109e-01,  4.8429e-02,  6.6756e-01,  1.2796e+00,  7.1916e-01,\n",
      "          1.5747e-01,  3.9296e-01,  5.5543e-01, -7.4247e-02, -3.2301e-02,\n",
      "          4.3658e-01,  3.7250e-01,  5.3799e-01, -1.1751e-01,  7.1511e-01,\n",
      "          5.8881e-01, -2.6202e-02,  4.7271e-02,  6.3797e-01,  2.5151e-01,\n",
      "          5.5669e-01, -1.0923e-02,  8.1644e-01, -3.7253e-01, -4.4384e-01,\n",
      "         -2.7434e-02,  3.0503e-01,  1.7436e-01, -3.1610e-01,  5.7835e-01,\n",
      "          7.5975e-01,  3.4881e-01,  3.0027e-01,  5.5354e-01, -2.0187e-01,\n",
      "          2.6095e-01, -8.1998e-02,  5.0498e-01,  5.3094e-01, -5.4719e-01,\n",
      "          4.3313e-01,  5.4749e-01,  1.8852e-01,  4.9520e-01,  9.9173e-02,\n",
      "          3.2069e-01,  3.4932e-01, -8.2360e-01,  3.6394e-01,  8.3210e-01,\n",
      "         -3.2562e-01,  5.3943e-01,  4.1817e-01, -8.3989e-02,  2.5717e-01,\n",
      "         -1.6448e-01, -6.3080e-01, -3.2727e-01,  3.3931e-01,  5.3726e-01,\n",
      "          6.3371e-01,  1.6988e-01,  4.6581e-01,  2.0502e-01, -4.6741e-01,\n",
      "         -8.7160e-01, -1.0435e+00, -5.7190e-01,  5.8437e-01, -1.1876e+00,\n",
      "         -1.2242e+00, -1.1305e+00, -1.0075e+00, -1.5035e+00, -7.5173e-01,\n",
      "         -4.0985e-01,  7.3429e-01,  7.2831e-01, -9.5549e-02,  2.0135e-01,\n",
      "          8.3619e-01, -1.4138e-01, -2.3917e-01, -6.5702e-01, -1.7287e+00,\n",
      "         -9.6930e-01, -1.1288e+00, -3.0571e-01, -9.0793e-01, -7.7689e-01,\n",
      "         -1.1219e+00, -1.0304e+00, -1.6598e+00, -4.1491e-01, -1.1720e-01,\n",
      "         -2.0204e+00, -7.7311e-01, -3.4646e-01, -2.9313e-01, -1.0871e+00,\n",
      "         -6.7616e-01,  3.9633e-01, -8.0801e-01, -1.3603e+00, -4.1695e-01,\n",
      "          3.6854e-01, -3.1891e-01, -4.6382e-01,  2.4246e-01,  5.8352e-01,\n",
      "         -6.7209e-01, -9.1442e-01, -1.4085e+00, -1.1651e+00, -7.0430e-01,\n",
      "         -1.6055e+00, -9.5657e-01, -1.3987e+00, -1.4663e+00, -1.5724e+00,\n",
      "         -1.5947e+00, -1.0700e+00, -1.6801e-02, -1.1908e-01, -4.0836e-01,\n",
      "          6.7244e-03, -2.6502e-01,  9.4466e-02, -1.2314e-01, -5.0077e-01,\n",
      "         -1.0136e+00, -1.8276e+00,  2.1440e-01,  7.6416e-01, -1.6456e+00,\n",
      "         -4.9278e-01,  7.5526e-01, -1.4144e-01, -1.0611e+00, -4.9680e-01,\n",
      "          6.3545e-01, -6.7928e-01, -1.4616e+00, -1.1044e-01, -1.1619e+00,\n",
      "         -1.0181e+00, -2.2579e+00, -1.4609e+00, -9.9689e-01, -9.4918e-01,\n",
      "          3.1615e-01,  8.1696e-01, -1.6208e-01,  5.0779e-01,  3.6642e-01,\n",
      "          5.9157e-02,  5.4808e-01,  5.4722e-02,  3.5456e-01, -9.9772e-02,\n",
      "         -4.7229e-01, -1.1987e+00, -6.2294e-01, -1.1862e+00, -8.4449e-01,\n",
      "         -7.3824e-01, -2.8614e-01, -2.3198e-01, -1.6290e-02, -5.8457e-01,\n",
      "         -1.2906e+00, -1.5495e+00,  2.6767e-01, -5.9324e-01, -6.1649e-01,\n",
      "          2.5750e-01, -4.6330e-01, -2.0351e-01, -5.8992e-01, -1.1292e+00,\n",
      "         -4.4120e-01, -1.1165e+00, -1.2573e+00, -1.4493e+00, -1.4894e-01,\n",
      "          6.0486e-01,  2.2126e-01, -1.3404e+00, -1.6664e+00, -5.0199e-01,\n",
      "          4.6339e-01, -9.3973e-01, -5.7955e-01,  4.3290e-01,  1.8811e-01,\n",
      "         -9.3797e-01,  8.3719e-01,  7.5899e-02, -2.2608e+00, -2.1669e+00,\n",
      "         -7.9325e-01,  1.3543e-01, -3.8723e-01, -3.8209e-01,  8.3278e-01,\n",
      "          6.7116e-02,  3.7154e-01,  2.0131e+00,  9.3552e-01,  4.4943e-01,\n",
      "          8.9188e-01, -5.0018e-01,  1.9191e-01,  2.0955e-01,  9.6960e-01,\n",
      "          1.0053e+00,  9.6367e-01, -1.1489e-01,  4.2429e-01,  3.4223e-01,\n",
      "         -1.1164e+00,  7.9908e-02,  1.2256e+00,  1.5238e+00,  4.0146e-01,\n",
      "         -1.0026e+00, -1.1020e-01,  1.9168e-01,  5.6375e-01,  3.3770e-01,\n",
      "          7.4661e-01, -3.7895e-01, -3.6867e-01,  5.4465e-01,  3.0560e-01,\n",
      "          1.0314e+00,  8.2028e-01, -6.8279e-02, -5.9807e-01, -5.3622e-01,\n",
      "         -1.0527e-03,  5.0768e-01,  1.4492e+00,  8.4815e-01, -8.3702e-01,\n",
      "         -3.0911e-01,  3.8453e-01,  8.1362e-01, -2.3986e-01, -2.5595e-01,\n",
      "          7.0561e-01,  1.3521e+00,  1.2383e+00,  7.4828e-02,  5.8442e-01,\n",
      "         -8.4188e-01,  6.6333e-01,  1.3954e+00,  2.3829e+00,  1.3593e+00,\n",
      "         -2.9548e-01, -1.5473e+00, -7.5324e-02, -7.1237e-02,  1.6459e+00,\n",
      "          1.1472e+00,  6.9084e-01, -9.2688e-02,  8.3671e-01, -2.6658e-01,\n",
      "         -7.4070e-02, -4.9421e-02,  9.3224e-01,  6.1848e-01,  2.8106e-01,\n",
      "          1.3209e-01,  1.4026e-01,  2.8768e-01, -7.5459e-01, -1.5491e+00,\n",
      "         -9.3111e-02, -4.8293e-01,  1.2526e+00,  1.6976e+00,  1.1018e+00,\n",
      "          6.6830e-01,  1.0711e+00,  6.9192e-01, -1.1711e+00,  1.0281e+00,\n",
      "         -9.7870e-01,  2.2404e-01, -4.9664e-01, -2.3611e-01,  1.3522e+00,\n",
      "         -1.8686e+00,  6.7209e-01,  1.2997e+00,  7.3950e-01,  1.2393e+00,\n",
      "          1.3649e+00,  9.6462e-01,  9.1901e-01,  3.6776e-01,  2.4730e-01,\n",
      "         -1.1573e+00, -1.0977e+00,  1.0571e+00,  5.2243e-01,  1.1692e+00,\n",
      "          1.8646e+00,  1.5860e-01,  1.5913e-01,  1.0749e+00,  5.4156e-01,\n",
      "         -9.7551e-01,  4.5649e-01,  1.0280e+00,  1.6541e+00,  3.4858e-01,\n",
      "         -5.3453e-01, -8.8076e-02, -3.4216e-01,  1.7187e-01, -8.4913e-02,\n",
      "          7.2708e-01,  1.8391e-01,  1.3631e-01, -8.8801e-01,  1.4614e-01,\n",
      "         -5.3009e-01, -6.1657e-01, -6.5217e-01,  3.7997e-02,  1.3514e+00,\n",
      "         -1.0109e+00,  1.3012e+00,  7.5562e-01,  8.1946e-01,  7.1774e-01,\n",
      "          8.2401e-01,  7.0714e-01, -2.2125e+00, -9.8432e-01, -1.8186e-01,\n",
      "         -3.7141e-01,  5.5254e-02,  6.2828e-01, -1.4905e-01, -1.6608e+00,\n",
      "         -5.1786e-01,  5.8614e-01,  4.1533e-01,  1.0858e+00,  9.6318e-01,\n",
      "         -2.7246e-01, -2.3158e-01,  9.7517e-01,  2.0446e-02, -1.2450e+00,\n",
      "         -1.0210e+00,  3.4135e-01,  1.1110e+00,  3.2540e-01, -2.1506e-01,\n",
      "          1.2191e+00,  1.5518e-01,  1.0296e+00, -9.0585e-01,  5.0046e-01,\n",
      "         -2.7371e-01, -7.7847e-01,  1.0297e+00,  3.2398e-01,  8.3328e-02,\n",
      "          3.6522e-01, -4.7605e-01,  8.3715e-01,  7.3409e-01,  1.2143e+00,\n",
      "          9.7062e-01, -2.2913e-01,  1.7142e+00,  1.1256e+00,  1.2038e+00,\n",
      "         -4.2067e-01,  6.3355e-01, -8.1605e-01,  7.8914e-01,  3.6454e-01,\n",
      "         -5.2575e-01,  1.0228e+00, -8.2445e-02, -5.5852e-01,  7.6665e-01,\n",
      "          2.3935e+00, -6.4090e-02, -3.0939e-01, -8.1679e-01,  3.3068e-01,\n",
      "         -7.6572e-02,  1.1935e+00, -7.0304e-01,  4.9472e-01, -1.5321e-01,\n",
      "          8.8654e-01,  5.1627e-01, -3.6940e-01,  8.0717e-01, -1.2226e-01,\n",
      "          2.0112e-01,  1.2095e+00,  6.3104e-01,  2.0345e+00,  8.5948e-01,\n",
      "          9.8365e-01,  7.0610e-01,  2.8776e-01,  6.5444e-01,  8.6059e-02,\n",
      "         -1.4313e+00,  1.3696e+00, -2.1922e-01, -1.1132e+00,  4.4046e-01,\n",
      "          2.8097e-01,  1.1291e+00,  5.0994e-01,  1.1422e+00, -4.8191e-01,\n",
      "          4.3003e-01,  1.2169e+00,  8.9359e-01,  4.9822e-01,  9.2289e-02,\n",
      "         -1.7781e+00,  1.2354e+00, -4.8789e-01,  1.3738e+00,  6.5573e-01,\n",
      "         -7.4259e-01,  7.1755e-01,  4.3551e-01, -4.4866e-01, -1.4172e+00,\n",
      "          1.0823e+00,  1.2444e-01,  7.6174e-01,  9.9154e-01, -5.7152e-02,\n",
      "          4.5168e-01,  4.3836e-02, -1.3232e-01,  1.0621e-01,  4.6077e-01,\n",
      "         -1.0153e-01, -1.1962e+00,  2.0763e-01, -7.0798e-01,  9.6264e-01,\n",
      "         -8.7683e-03,  1.1918e+00,  4.0344e-01, -7.9215e-01, -5.8299e-01,\n",
      "          1.8820e-01, -3.8116e-01, -2.0054e-01,  5.3711e-01,  1.5605e+00,\n",
      "         -7.4279e-01,  1.7192e+00,  1.0280e+00,  9.2612e-01,  2.4966e-01,\n",
      "          7.9805e-01,  4.2691e-01, -5.2308e-01,  5.9939e-01,  6.5983e-01,\n",
      "         -1.6201e+00, -4.2385e-02, -1.0879e+00, -1.4170e-01, -7.6619e-01,\n",
      "         -6.3821e-01,  7.7346e-01,  1.0779e+00,  4.2893e-01, -8.0280e-01,\n",
      "          1.0571e+00,  1.7005e+00, -9.3742e-03, -3.9153e-01,  5.2230e-01,\n",
      "          1.9712e+00, -2.5371e-01, -6.1595e-01,  5.9055e-01,  4.5730e-01,\n",
      "         -5.4894e-01, -2.1364e-01,  2.6063e-01,  7.1418e-01,  2.4546e-01,\n",
      "          7.3478e-01,  1.1124e+00,  1.8196e-01, -4.4603e-01,  3.2797e-01,\n",
      "         -3.0202e-01,  5.5981e-01, -8.9278e-01, -4.7863e-01,  6.9703e-01,\n",
      "          1.8201e-01,  2.5876e-01,  1.2170e+00,  1.9134e-01, -7.4111e-01,\n",
      "          1.3297e+00, -3.6378e-01, -1.1928e-01,  1.3913e+00, -4.9382e-01,\n",
      "         -4.1229e-02,  2.2316e+00, -7.2256e-01,  1.7542e+00, -1.5050e+00,\n",
      "          2.3444e-01, -7.2876e-02,  7.3902e-01,  1.1372e+00, -3.3333e-02,\n",
      "          1.2969e+00,  3.7053e-02,  4.6458e-01,  2.8320e-01,  5.8562e-01,\n",
      "          2.9355e-02, -1.7117e-01,  6.4177e-01,  6.9758e-01,  1.4723e+00,\n",
      "          6.1050e-01, -1.7756e-01,  1.8441e-01,  3.6134e-01,  7.2002e-01,\n",
      "         -6.0547e-01,  9.4056e-01, -1.4305e-01,  1.4158e+00,  1.5432e-02,\n",
      "          1.9760e-01,  8.1597e-01,  5.8202e-01,  9.8945e-01,  1.4939e+00,\n",
      "          6.2634e-01,  5.9263e-01,  5.0703e-01, -5.3864e-01,  1.4291e+00,\n",
      "          4.4632e-01,  4.5858e-01,  1.7303e+00,  8.8576e-01,  9.1700e-01,\n",
      "          4.8995e-01,  6.2439e-01,  9.1419e-01,  1.1551e+00, -9.5336e-01,\n",
      "         -1.1218e+00, -8.1219e-01,  9.7376e-01,  9.4836e-01,  1.8726e+00,\n",
      "          2.3560e-01,  5.2663e-01,  1.2612e+00,  3.7139e-01, -2.1337e-01,\n",
      "          7.6932e-01,  9.9900e-01,  1.7092e+00,  1.0384e+00,  3.0110e-01,\n",
      "          1.2495e-01,  1.0136e+00,  1.0114e+00, -6.4002e-01,  3.1939e-01,\n",
      "         -5.5785e-01,  2.4966e-01, -9.9809e-01, -1.1306e+00,  9.2209e-01,\n",
      "          7.4896e-01,  3.5546e-01,  1.9431e-01,  1.5801e+00,  5.1825e-02,\n",
      "         -3.7793e-01,  1.1240e+00, -2.6236e-01,  1.7636e+00, -1.1313e+00,\n",
      "         -1.0515e-01,  3.5514e-01, -1.2984e+00,  1.6664e+00,  5.8806e-01,\n",
      "         -1.5894e+00, -9.7565e-01,  2.7091e-01,  8.1973e-01,  1.0201e+00,\n",
      "         -9.1892e-01,  2.9060e-01,  1.0904e+00,  1.8009e+00, -6.4090e-01,\n",
      "          1.1333e+00,  2.7585e-01, -5.4917e-01, -1.0118e+00,  2.0418e-01,\n",
      "          6.0104e-01,  1.5900e+00,  1.6610e+00,  1.3475e+00, -6.8202e-01,\n",
      "          1.5728e+00,  3.4425e-01,  2.4649e-01,  4.0117e-01,  9.1652e-01,\n",
      "          1.7999e+00,  8.3741e-01, -4.4728e-01,  4.0224e-01,  1.0176e+00,\n",
      "          1.3025e+00,  1.2979e+00,  1.6760e+00, -5.2527e-01, -6.2040e-01,\n",
      "          1.1479e+00, -4.3392e-01, -3.3929e-02, -7.5022e-01,  1.0729e+00,\n",
      "          2.7856e-01,  1.2628e+00,  9.0112e-01, -8.0742e-02, -1.8869e-01,\n",
      "          5.2683e-01, -1.1658e-02, -3.3318e-01,  1.6761e+00, -2.2591e-01,\n",
      "          8.7886e-01, -1.5889e+00,  1.1486e+00, -8.6086e-01, -2.5349e+00,\n",
      "          6.5360e-01,  1.1718e+00, -3.4976e-01, -1.4145e-01,  1.7829e+00,\n",
      "          1.1334e+00, -4.3595e-01,  8.0104e-01,  1.3892e+00,  3.1223e-01,\n",
      "          2.3744e-01, -2.6682e-01, -2.6828e-01, -1.1603e+00,  3.6789e-01,\n",
      "         -7.5916e-01,  3.7576e-01,  5.2592e-01,  2.4193e-01, -7.3078e-01,\n",
      "         -9.7214e-01,  1.0200e+00,  4.4956e-01,  1.9599e+00,  2.0032e+00,\n",
      "         -1.3479e+00, -1.9329e-01,  1.8294e+00,  8.2255e-01,  7.6937e-01,\n",
      "         -1.8565e-01, -2.9995e-01,  1.4653e+00, -1.3989e+00,  1.0637e+00,\n",
      "          1.2433e+00,  1.1746e+00,  6.2168e-01, -5.2703e-01, -2.0180e+00,\n",
      "         -5.0958e-01,  1.2883e-01,  3.9095e-01,  4.0853e-01,  1.9597e-01,\n",
      "         -6.8518e-02,  9.6933e-01, -6.0850e-01,  7.5226e-01, -3.1957e-01,\n",
      "         -9.3652e-01, -1.0395e+00, -4.9298e-01,  1.5769e-02,  1.3972e+00,\n",
      "         -1.4370e-01, -3.8051e-02,  7.2856e-01, -1.7762e+00, -6.8910e-02,\n",
      "         -5.5032e-01,  3.6411e-01,  1.4983e-01,  1.1600e-01,  3.2235e-01,\n",
      "         -2.7246e-01, -7.6475e-01, -3.9783e-01,  3.0531e-01, -4.1467e-01,\n",
      "         -6.1503e-01, -9.5870e-01,  5.1010e-01,  7.0882e-01, -1.4666e-01,\n",
      "          1.6963e-01, -4.5147e-01, -4.6271e-01,  9.4248e-02,  8.6274e-01,\n",
      "         -3.7080e-01, -3.5520e-01, -4.1307e-01, -4.7486e-02, -6.7971e-01,\n",
      "          2.9688e-01,  2.3211e-01, -4.3347e-01, -7.3737e-01, -1.2555e+00,\n",
      "         -8.1523e-02,  6.3748e-01, -6.6479e-01,  7.9410e-01,  2.8010e-01,\n",
      "         -8.6588e-02,  1.0760e+00, -2.5121e-01, -1.5412e-01, -2.0387e+00,\n",
      "          9.5593e-01, -1.5050e+00,  4.3709e-01,  2.2445e-01, -6.9663e-01,\n",
      "         -4.6468e-01,  7.0820e-02,  4.4819e-01, -3.4184e-01, -9.9388e-01,\n",
      "         -1.3220e+00, -1.8301e+00,  1.2951e+00, -1.5632e-01, -5.2270e-01,\n",
      "          8.3333e-02, -1.2768e+00, -8.0676e-01, -1.7908e+00, -5.7651e-01,\n",
      "         -2.5661e-01,  3.9485e-01, -1.7371e-01,  1.2377e+00,  7.4679e-01]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "prediction = model(data) # forward pass\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用模型的预测和相应的标签来计算误差（loss）。 下一步是通过网络反向传播此误差。 当我们在误差张量上调用 ```.backward()``` 时，开始反向传播。 然后，Autograd 会为每个模型参数计算梯度并将其存储在参数的 ```.grad``` 属性中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: -494.68878173828125\n"
     ]
    }
   ],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "print(f\"loss: {loss}\")\n",
    "loss.backward() # backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们加载一个优化器，在本例中为 SGD，学习率为 0.01，动量为 0.9。 我们在优化器中注册模型的所有参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们调用 ```.step()``` 启动梯度下降。 优化器通过 ```.grad``` 中存储的梯度来调整每个参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step() #gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Autograd 的微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们来看看 ```autograd``` 如何收集梯度。 我们用 ```requires_grad=True``` 创建两个张量$a$和$b$。 这向 ```autograd``` 发出信号，应跟踪对它们的所有操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们从a和b创建另一个张量Q。\n",
    "\n",
    "$Q = 3*a ^{3}-b^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设a和b是神经网络的参数，Q是误差。 在 NN 训练中，我们想要相对于参数的误差，即\n",
    "\n",
    "$\\frac {\\alpha Q}{\\alpha a} = 9a^{2}$\n",
    "\n",
    "$\\frac {\\alpha Q}{\\alpha b} = -2b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们在$Q$上调用 ```.backward()```时， ```Autograd``` 将计算这些梯度并将其存储在各个张量的 ```.grad``` 属性中。\n",
    "\n",
    "我们需要在 ```Q.backward()``` 中显式传递 ```gradient``` 参数，因为它是向量。 ```gradient``` 是与$Q$形状相同的张量，它表示$Q$相对于本身的梯度，即\n",
    "\n",
    "$ \\frac{dQ}{dQ} = 1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样，我们也可以将Q聚合为一个标量，然后隐式地向后调用，例如```Q.sum().backward()```。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度现在沉积在$a.grad$和$b.grad$中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "# check if collected gradients are correct\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.可选阅读-使用autograd的向量微积分\n",
    "\n",
    "从数学上讲，如果您具有向量值函数$y = f(x)$，则$y$相对于$x$的雅可比矩阵$J$：\n",
    "\n",
    "![1.2.1](1.2.1.png)\n",
    "\n",
    "一般来说，torch.autograd是用于计算向量雅可比积的引擎。 也就是说，给定任何向量$v$，计算乘积$J^T · v$\n",
    "\n",
    "如果v恰好是标量函数的梯度\n",
    "\n",
    "![1.2.2](1.2.2.png)\n",
    "\n",
    "然后根据链式规则，向量-雅可比积将是l相对于x的梯度：\n",
    "\n",
    "![1.2.3](1.2.3.png)\n",
    "\n",
    "上面的示例中使用的是 vector-Jacobian 乘积的这一特征。 ```external_grad```表示$v$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.计算图\n",
    "\n",
    "从概念上讲， Autograd 在由函数对象组成的有向无环图$（DAG）$中记录数据（张量）和所有已执行的操作（以及由此产生的新张量）。 在此 DAG 中，叶子是输入张量，根是输出张量。 通过从根到叶跟踪此图，可以使用链式规则自动计算梯度。\n",
    "\n",
    "在正向传播中， Autograd 同时执行两项操作：\n",
    "\n",
    "> * 运行请求的操作以计算结果张量，并且\n",
    "> * 在 DAG 中维护操作的 *梯度函数* 。\n",
    "\n",
    "当在 DAG 根目录上调用 ```.backward()``` 时，反向传递开始。 ```autograd``` 然后：\n",
    "\n",
    "> 1. 从每个 ```.grad_fn``` 计算梯度，\n",
    "> 2. 将它们累积在各自的张量的 ```.grad``` 属性中，然后\n",
    "> 3. 使用链式规则，一直传播到叶子张量。\n",
    "\n",
    "下面是我们示例中 DAG 的直观表示。 在图中，箭头指向前进的方向。 节点代表正向传播中每个操作的反向函数。 蓝色的叶节点代表我们的叶张量 $a$ 和 $b$ 。\n",
    "\n",
    "![1.2.4](1.2.4.png)\n",
    "\n",
    "注意\n",
    "\n",
    "**DAG 在 PyTorch 中是动态的**。要注意的重要一点是，图是从头开始重新创建的； 在每个 ```.backward()``` 调用之后，Autograd 开始填充新图。 这正是允许您在模型中使用控制流语句的原因。 您可以根据需要在每次迭代中更改形状，大小和操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.从 DAG 中排除\n",
    "\n",
    "```torch.autograd``` 跟踪所有将其 ```requires_grad``` 标志设置为 ```True``` 的张量的操作。 对于不需要梯度的张量，将此属性设置为 ```False``` 会将其从梯度计算 DAG 中排除。\n",
    "\n",
    "即使只有一个输入张量具有 ```requires_grad=True``` ，操作的输出张量也将需要梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does `a` require gradients? : False\n",
      "Does `b` require gradients?: True\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad = True)\n",
    "\n",
    "a = x + y \n",
    "print(f\"Does `a` require gradients? : {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients?: {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 NN 中，不计算梯度的参数通常称为**冻结参数**。 如果事先知道您不需要这些参数的梯度，则“冻结”模型的一部分很有用（通过减少自动梯度计算，这会带来一些性能优势）。\n",
    "\n",
    "从 DAG 中排除很重要的另一个常见用例是 ```调整预训练网络```\n",
    "\n",
    "在微调中，我们冻结了大部分模型，通常仅修改分类器层以对新标签进行预测。 让我们来看一个小例子来说明这一点。 和以前一样，我们加载一个预训练的 resnet18 模型，并冻结所有参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all the parameters in the network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们要在具有 10 个标签的新数据集中微调模型。 在 resnet 中，分类器是最后一个线性层model.fc。 我们可以简单地将其替换为充当我们的分类器的新线性层（默认情况下未冻结）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，除了 ```model.fc``` 的参数外，模型中的所有参数都将冻结。 计算梯度的唯一参数是model.fc的权重和偏差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize only the classifier\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，尽管我们在优化器中注册了所有参数，但唯一可计算梯度的参数（因此会在梯度下降中进行更新）是分类器的权重和偏差。\n",
    "\n",
    "```torch.no_grad()``` 中的上下文管理器可以使用相同的排除功能。"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7bd64dcbcd4ea4cac8525d7517ab7632fba054172e77ca3ecafea3ce7509697"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
