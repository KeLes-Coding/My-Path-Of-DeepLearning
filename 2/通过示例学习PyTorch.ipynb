{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 通过示例学习PyTorch\n",
    "\n",
    "本教程通过独立的示例介绍 ```<u>PyTorch<\\u>``` 的基本概念。\n",
    "\n",
    "PyTorch 的核心是提供两个主要功能：\n",
    "> * n 维张量，类似于 NumPy，但可以在 GPU 上运行\n",
    "> * 用于构建和训练神经网络的自动微分\n",
    "\n",
    "我们将使用将三阶多项式拟合 ```y = sin(x)``` 的问题作为运行示例。 该网络将具有四个参数，并且将通过使网络输出与实际输出之间的欧几里德距离最小化来进行梯度下降训练，以适应随机数据。\n",
    "\n",
    "注意\n",
    "\n",
    "您可以在```本页```浏览各个示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ⅰ.预热：NumPy\n",
    "\n",
    "在介绍 PyTorch 之前，我们将首先使用 numpy 实现网络。\n",
    "\n",
    "Numpy 提供了一个 n 维数组对象，以及许多用于操纵这些数组的函数。 Numpy 是用于科学计算的通用框架。 它对计算图，深度学习或梯度一无所知。 但是，通过使用 numpy 操作手动实现网络的前向和后向传递，我们可以轻松地使用 numpy 使三阶多项式适合正弦函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 3267.156712057645\n",
      "199 2232.6351478785537\n",
      "299 1528.3182050261917\n",
      "399 1048.2632717706554\n",
      "499 720.6895515642644\n",
      "599 496.9076593771385\n",
      "699 343.8553352089778\n",
      "799 239.05704537173096\n",
      "899 167.21697048762707\n",
      "999 117.91371822292233\n",
      "1099 84.03887062103483\n",
      "1199 60.738232691594746\n",
      "1299 44.69316361335828\n",
      "1399 33.63221626564388\n",
      "1499 25.9989097818173\n",
      "1599 20.725463614579134\n",
      "1699 17.07852244400094\n",
      "1799 14.553846410028086\n",
      "1899 12.80434305153441\n",
      "1999 11.590828192638812\n",
      "Result: y = -0.04840604083464015 + 0.83134880554618 x + 0.008350847523971711 x^2 + -0.0897185928181053 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "# 随机初始化权重\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    # 正向传递：计算预测 y\n",
    "    # y = a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ⅱ.PyTorch：张量\n",
    "\n",
    "Numpy 是一个很棒的框架，但是它不能利用 GPU 来加速其数值计算。 对于现代深度神经网络，GPU 通常会提供 ```50 倍或更高``` 的加速，因此遗憾的是，numpy 不足以实现现代深度学习。\n",
    "\n",
    "在这里，我们介绍最基本的 PyTorch 概念：**张量**。 PyTorch 张量在概念上与 numpy 数组相同：张量是 n 维数组，PyTorch 提供了许多在这些张量上进行操作的函数。 在幕后，张量可以跟踪计算图和梯度，但它们也可用作科学计算的通用工具。\n",
    "\n",
    "与 numpy 不同，PyTorch 张量可以利用 GPU 加速其数字计算。 要在 GPU 上运行 PyTorch 张量，您只需要指定正确的设备即可。\n",
    "\n",
    "在这里，我们使用 PyTorch 张量将三阶多项式拟合为正弦函数。 像上面的 numpy 示例一样，我们需要手动实现通过网络的正向和反向传递："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1455.956787109375\n",
      "199 991.6073608398438\n",
      "299 676.9944458007812\n",
      "399 463.6046447753906\n",
      "499 318.713134765625\n",
      "599 220.22357177734375\n",
      "699 153.20050048828125\n",
      "799 107.5394515991211\n",
      "899 76.39625549316406\n",
      "999 55.13068389892578\n",
      "1099 40.593284606933594\n",
      "1199 30.643871307373047\n",
      "1299 23.826614379882812\n",
      "1399 19.15015411376953\n",
      "1499 15.938590049743652\n",
      "1599 13.73052978515625\n",
      "1699 12.210708618164062\n",
      "1799 11.16346263885498\n",
      "1899 10.44105339050293\n",
      "1999 9.9421968460083\n",
      "Result: y = 0.02958722785115242 + 0.8387191891670227 x + -0.005104289390146732 x^2 + -0.09076696634292603 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU()  (取消注释以在 GPU 上运行)\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "# 随机初始化权重\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    # 正向传递：计算预测 y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    # 反向螺旋体，用于计算 a、b、c、d 相对于损耗的梯度\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    # 使用梯度下降更新权重\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ⅲ.PyTorch：张量 与 Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们准备一个三阶多项式，通过最小化平方欧几里得距离来训练，并预测函数 ```y = sin(x)``` 在 ```-pi``` 到 ```pi``` 上的值。\n",
    "\n",
    "此实现使用了 PyTorch 张量(tensor)运算来实现前向传播，并使用 PyTorch Autograd 来计算梯度。\n",
    "\n",
    "PyTorch 张量表示计算图中的一个节点。 如果x是一个张量，且 ```x.requires_grad=True``` ，则 ```x.grad``` 是另一个张量，它保存了x相对于某个标量值的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1503.9141845703125\n",
      "199 1011.8036499023438\n",
      "299 682.1997680664062\n",
      "399 461.2796325683594\n",
      "499 313.0946044921875\n",
      "599 213.62030029296875\n",
      "699 146.7905731201172\n",
      "799 101.8546142578125\n",
      "899 71.61383056640625\n",
      "999 51.24409866333008\n",
      "1099 37.51081848144531\n",
      "1199 28.242835998535156\n",
      "1299 21.982315063476562\n",
      "1399 17.74897003173828\n",
      "1499 14.88352108001709\n",
      "1599 12.941917419433594\n",
      "1699 11.624869346618652\n",
      "1799 10.730536460876465\n",
      "1899 10.122562408447266\n",
      "1999 9.708781242370605\n",
      "Result: y = -0.022038761526346207 + 0.8359349966049194 x + 0.0038020508363842964 x^2 + -0.09037093073129654 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "# 创建张量来保存输入和输出。\n",
    "# 默认情况下，requires_grad=False，这表明在向后传递期间，我们不需要计算相对于这些张量的梯度。\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "# 为权重创建随机张量。\n",
    "# 对于三阶多项式，我们需要 4 个权重：y = a + b x + c x^2 + d x^3 \n",
    "# 设置 requires_grad=True 表示我们要在向后传递期间计算相对于这些张量的梯度。\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    # 正向传递：使用张量上的运算计算预测的 y。\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    # loss.item（） 获取损失中持有的标量值。\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    # 使用自动渐变来计算向后传递。\n",
    "    # 此调用将计算相对于所有张量的损失梯度，requires_grad=True。\n",
    "    # 在此调用a.grad，b.grad之后。\n",
    "    # c.grad 和 d.grad 将分别是保持相对于 a、b、c、d 的损失梯度的张量。\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # 使用梯度下降手动更新权重。\n",
    "    # 换行torch.no_grad（），因为权重requires_grad=True，但我们不需要在自动升级中跟踪它。\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        # 更新权重后手动清零渐变\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ⅳ.PyTorch：定义新的 Autograd 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们准备一个三阶多项式，通过最小化平方欧几里得距离来训练，并预测函数<font color=\"orange\"> $y = sin(x)$ </font>在<font color=\"orange\"> $-pi$ </font>到<font color=\"orange\"> $pi$ </font>上的值。\n",
    "\n",
    "这里我们不将多项式写为<font color=\"orange\"> $y = a + bx + cx^2 + dx^3$ </font>，而是将多项式写为<font color=\"orange\"> $y = a + bP_3(c + dx)$ </font>，其中<font color=\"orange\"> $P_3(x) = 1/2 (5x ^ 3 - 3x)$ </font>是```三次勒让德多项式。```\n",
    "\n",
    "此实现使用了 PyTorch 张量(tensor)运算来实现前向传播，并使用 PyTorch Autograd 来计算梯度。\n",
    "\n",
    "在此实现中，我们实现了自己的自定义 Autograd 函数来执行<font color=\"orange\"> $P'_3(x)$ </font>。 从数学定义上讲，<font color=\"orange\">$ P'_3(x) = 3/2 (5x ^ 2 - 1)$</font> ："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 209.95834350585938\n",
      "199 144.66018676757812\n",
      "299 100.70249938964844\n",
      "399 71.03520202636719\n",
      "499 50.978515625\n",
      "599 37.40313720703125\n",
      "699 28.20686912536621\n",
      "799 21.973186492919922\n",
      "899 17.745729446411133\n",
      "999 14.877889633178711\n",
      "1099 12.931766510009766\n",
      "1199 11.610918998718262\n",
      "1299 10.714248657226562\n",
      "1399 10.105474472045898\n",
      "1499 9.692106246948242\n",
      "1599 9.411375045776367\n",
      "1699 9.220745086669922\n",
      "1799 9.091285705566406\n",
      "1899 9.003361701965332\n",
      "1999 8.943639755249023\n",
      "Result: y = -1.765793067320942e-11 + -2.208526849746704 * P3(9.924167737596079e-11 + 0.2554861009120941 x)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "\n",
    "    我们可以通过子类化 torch.autograd.Function \n",
    "    并实现在 Tensors 上运行的向前和向后传递\n",
    "    来实现我们自己的自定义 autograd 函数。\n",
    "    \"\"\"\n",
    "\n",
    "    # python staticmethod 返回函数的静态方法\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "\n",
    "        在正向传递中，我们接收到包含输入的张量，并返回包含输出的张量。\n",
    "        ctx 是一个上下文对象，可用于隐藏信息以进行向后计算。\n",
    "        您可以使用ctx.save_for_backward方法缓存任意对象，以便在向后传递中使用。\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "\n",
    "        在反向传递中，我们收到一个包含相对于输出的损耗梯度的张量，并且我们需要计算相对于输入的损耗梯度。\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n",
    "\n",
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "# 创建张量来保存输入和输出。\n",
    "# 默认情况下，requires_grad=False，\n",
    "# 这表明在向后传递期间，我们不需要计算相对于这些张量的梯度。\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For this example, we need\n",
    "# 4 weights: y = a + b * P3(c + d * x), these weights need to be initialized\n",
    "# not too far from the correct result to ensure convergence.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "# 为权重创建随机张量。\n",
    "# 对于此示例，我们需要 4 个权重：y = a + b * P3（c + d * x），这些权重需要在距离正确结果不太远的地方进行初始化，以确保收敛。\n",
    "# 设置 requires_grad=True 表示我们希望在向后传递期间计算相对于这些张量的梯度。\n",
    "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 5e-6\n",
    "for t in range(2000):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'P3'.\n",
    "    # 为了应用我们的函数，我们使用Function.apply方法。我们将其别名为\"P3\"。\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # P3 using our custom autograd operation.\n",
    "    # 正向传递：使用运算计算预测的y;\n",
    "    #         我们使用自定义自动升级操作计算 P3。\n",
    "    y_pred = a + b * P3(c + d * x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ⅴ.PyTorch: nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个三阶多项式，通过最小化平方欧几里得距离来训练，并预测函数<font color = \"orange\"> $y = sin(x)$ </font>在<font color = \"orange\"> $-pi$ </font>到<font color = \"orange\"> $pi$ </font>上的值。\n",
    "\n",
    "这个实现使用 PyTorch 的 <font color = \"orange\">nn</font> 包来构建神经网络。 PyTorch Autograd 让我们定义计算图和计算梯度变得容易了，但是原始的 Autograd 对于定义复杂的神经网络来说可能太底层了。 这时候 <font color = \"orange\">nn</font> 包就能帮上忙。 <font color = \"orange\">nn</font> 包定义了一组模块，你可以把它视作一层神经网络，该神经网络层接受输入，产生输出，并且可能有一些可训练的权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 288.37646484375\n",
      "199 198.5834503173828\n",
      "299 137.7730712890625\n",
      "399 96.54682159423828\n",
      "499 68.56768798828125\n",
      "599 49.55795669555664\n",
      "699 36.627952575683594\n",
      "799 27.823333740234375\n",
      "899 21.821151733398438\n",
      "999 17.724679946899414\n",
      "1099 14.925694465637207\n",
      "1199 13.011001586914062\n",
      "1299 11.699724197387695\n",
      "1399 10.800652503967285\n",
      "1499 10.18351936340332\n",
      "1599 9.759430885314941\n",
      "1699 9.467667579650879\n",
      "1799 9.266709327697754\n",
      "1899 9.128161430358887\n",
      "1999 9.032530784606934\n",
      "Result: y = -0.012886816635727882 + 0.8487757444381714 x + 0.0022231899201869965 x^2 + -0.09219741821289062 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "# 对于此示例，输出 y 是 （x， x^2， x^3） 的线性函数，\n",
    "# 因此我们可以将其视为线性层神经网络。\n",
    "# 让我们准备张量（x， x^2， x^3）。\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
    "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
    "# of shape (2000, 3)\n",
    "# 在上面的代码中，\n",
    "# x.unsqueeze（-1） 有 shape （2000， 1），\n",
    "# p 有 shape （3，），\n",
    "# 对于这种情况，广播语义将应用于获得形状的张量 （2000， 3）\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. The Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
    "# to match the shape of `y`.\n",
    "# 使用 nn 包将我们的模型定义为一系列层。\n",
    "# 嗯。顺序是一个模块，其中包含其他模块，并按顺序应用它们以生成其输出。\n",
    "# 线性模块使用线性函数计算输入输出，并保存其权重和偏差的内部张量。\n",
    "# 展平层将线性层的输出平展为 1D 张量，以匹配 \"y\" 的形状。\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "# nn 包还包含常用损失函数的定义;在这种情况下，我们将使用均方误差（MSE）作为我们的损失函数。\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    # 正向传递：通过将 x 传递给模型来计算预测的 y。\n",
    "    #         模块对象将覆盖__call__运算符，以便您可以像调用函数一样调用它们。\n",
    "    #         这样做时，您将输入数据的张量传递给模块，它会生成输出数据的张量。\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    # 计算和打印丢失。\n",
    "    # 我们传递包含 y 的预测值和真实值的张量，损失函数返回包含损失的张量。\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    # 在运行向后通道之前将渐变清零。\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    # 向后传递：计算相对于模型所有可学习参数的损失梯度。\n",
    "    # 在内部，每个模块的参数都存储在张量中，requires_grad=True，因此此调用将计算模型中所有可学习参数的梯度。\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    # 使用梯度下降更新权重。\n",
    "    # 每个参数都是一个张量，因此我们可以像以前一样访问其梯度\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "# 您可以访问\"模型\"的第一层，就像访问列表的第一项一样\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "# 对于线性层，其参数存储为\"权重\"和\"偏差\"。\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ⅵ.PyTorch: optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过训练的三阶多项式，可以通过最小化平方的欧几里得距离来预测 <font color=\"orange\">$y = sin(x)$</font>从 <font color=\"orange\">$-pi$</font> 到 <font color=\"orange\">$pi$</font>。\n",
    "\n",
    "此实现使用来自 PyTorch 的 <font color=\"orange\">nn</font> 包来构建网络。\n",
    "\n",
    "与其像以前那样手动更新模型的权重，不如使用 <font color=\"orange\">optim</font> 包定义一个优化器，该优化器将为我们更新权重。 <font color=\"orange\">optim</font> 包定义了许多深度学习常用的优化算法，包括 SGD + 动量，RMSProp，Adam 等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 6489.7978515625\n",
      "199 1493.303955078125\n",
      "299 313.51251220703125\n",
      "399 123.85812377929688\n",
      "499 70.52151489257812\n",
      "599 38.60538864135742\n",
      "699 20.43478775024414\n",
      "799 11.971320152282715\n",
      "899 9.273175239562988\n",
      "999 8.841493606567383\n",
      "1099 8.817790985107422\n",
      "1199 8.817176818847656\n",
      "1299 8.817168235778809\n",
      "1399 9.030593872070312\n",
      "1499 8.940629959106445\n",
      "1599 9.032800674438477\n",
      "1699 9.050511360168457\n",
      "1799 8.937700271606445\n",
      "1899 8.88145637512207\n",
      "1999 8.905050277709961\n",
      "Result: y = -0.00046780623961240053 + 0.8571969866752625 x + -0.00046821858268231153 x^2 + -0.09287428855895996 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Prepare the input tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use RMSprop; the optim package contains many other\n",
    "# optimization algorithms. The first argument to the RMSprop constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "\n",
    "linear_layer = model[0]\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ⅶ.PyTorch:自定义nn模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过训练的三阶多项式，可以通过最小化平方的欧几里得距离来预测 <font color=orange>$y = sin(x)$</font> 从 <font color=orange>$-pi$</font> 到 <font color=orange>$pi$</font> 。\n",
    "\n",
    "此实现将模型定义为自定义 <font color=orange>$Module$</font> 子类。 每当您想要一个比现有模块的简单序列更复杂的模型时，都需要以这种方式定义模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 108.68385314941406\n",
      "199 74.85714721679688\n",
      "299 52.488563537597656\n",
      "399 37.696712493896484\n",
      "499 27.91497802734375\n",
      "599 21.446483612060547\n",
      "699 17.16891860961914\n",
      "799 14.340225219726562\n",
      "899 12.469612121582031\n",
      "999 11.232577323913574\n",
      "1099 10.414496421813965\n",
      "1199 9.873517036437988\n",
      "1299 9.515730857849121\n",
      "1399 9.279145240783691\n",
      "1499 9.12269115447998\n",
      "1599 9.019227027893066\n",
      "1699 8.950798034667969\n",
      "1799 8.905545234680176\n",
      "1899 8.875618934631348\n",
      "1999 8.855826377868652\n",
      "Result: y = 0.0003440560249146074 + 0.8627766966819763 x + -5.9356367273721844e-05 x^2 + -0.09418893605470657 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class Polynomial3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate four parameters and assign them as\n",
    "        member parameters.\n",
    "\n",
    "        在构造函数中，我们实例化四个参数，并将它们作为成员参数赋值。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "\n",
    "        在正向函数中，我们接受输入数据的张量，并且我们必须返回输出数据的张量。\n",
    "        我们可以使用构造函数中定义的模块以及张量上的任意运算符。\n",
    "        \"\"\"\n",
    "        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
    "\n",
    "        就像Python中的任何类一样，你也可以在PyTorch模块上定义自定义方法。\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "# 通过实例化上面定义的类来构造我们的模型\n",
    "model = Polynomial3()\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the nn.Linear\n",
    "# module which is members of the model.\n",
    "# 构建我们的损失函数和优化器。\n",
    "# SGD 构造函数中对 model.parameters（） 的调用将包含 nn 的可学习参数。\n",
    "# 作为模型成员的线性模块。\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
    "for t in range(2000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ⅷ.控制流+权重共享"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了展示 PyTorch 动态图的强大功能，我们将实现一个非常奇怪的模型：一个从三到五阶（动态变化）的多项式，在每次正向传递中选择一个 3 到 5 之间的一个随机数，并将这个随机数作为阶数，第四和第五阶共用同一个权重来多次重复计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 1795.8043212890625\n",
      "3999 817.8870239257812\n",
      "5999 476.62420654296875\n",
      "7999 196.5718231201172\n",
      "9999 99.62507629394531\n",
      "11999 51.38410949707031\n",
      "13999 29.43459701538086\n",
      "15999 18.814685821533203\n",
      "17999 13.621737480163574\n",
      "19999 11.303443908691406\n",
      "21999 10.935626983642578\n",
      "23999 9.355867385864258\n",
      "25999 8.912134170532227\n",
      "27999 8.956872940063477\n",
      "29999 8.663052558898926\n",
      "Result: y = -0.007251318544149399 + 0.8541530966758728 x + 0.0008736016461625695 x^2 + -0.09326013177633286 x^3 + 9.603642683941871e-05 x^4 ? + 9.603642683941871e-05 x^5 ?\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate five parameters and assign them as members.\n",
    "\n",
    "        在构造函数中，我们实例化五个参数，并将它们作为成员进行分配。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        self.e = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 4, 5\n",
    "        and reuse the e parameter to compute the contribution of these orders.\n",
    "        对于模型的正向传递，我们随机选择 4、5，然后重用 e 参数来计算这些订单的贡献。\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "        由于每个正向传递都构建了一个动态计算图，因此在定义模型的前向传递时，我们可以使用普通的 Python 控制流运算符，如循环或条件语句。\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same parameter many\n",
    "        times when defining a computational graph.\n",
    "        在这里，我们还看到，在定义计算图时，多次重用相同的参数是完全安全的。\n",
    "        \"\"\"\n",
    "        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "        for exp in range(4, random.randint(4, 6)):\n",
    "            y = y + self.e * x ** exp\n",
    "        return y\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
    "\n",
    "        就像Python中的任何类一样，你也可以在PyTorch模块上定义自定义方法。\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "# 通过实例化上面定义的类来构造我们的模型\n",
    "model = DynamicNet()\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "# 构建我们的损失函数和优化器。用香草随机梯度下降训练这个奇怪的模型是很困难的，所以我们使用动量\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\n",
    "for t in range(30000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 2000 == 1999:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7bd64dcbcd4ea4cac8525d7517ab7632fba054172e77ca3ecafea3ce7509697"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
